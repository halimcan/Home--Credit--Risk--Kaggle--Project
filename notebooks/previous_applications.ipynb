{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1764068269034,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "EXfj0ZXAJQPY"
   },
   "outputs": [],
   "source": [
    "# Installments of required tables\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 88312,
     "status": "ok",
     "timestamp": 1764068357384,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "OtP8WjB8KmIR"
   },
   "outputs": [],
   "source": [
    "#  BigQuery API activation\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9167,
     "status": "ok",
     "timestamp": 1764068366585,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "2oOvouFgKr87"
   },
   "outputs": [],
   "source": [
    "# big query add-on installation\n",
    "\n",
    "!pip install --quiet google-cloud-bigquery\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 189411,
     "status": "ok",
     "timestamp": 1764068556026,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "t0eN3TYQKtZK"
   },
   "outputs": [],
   "source": [
    "# BigQuery client initiation\n",
    "client = bigquery.Client(project=\"homecredit-478707\")\n",
    "\n",
    "# From BigQuery\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM `homecredit-478707.Homecredit_Tables.previous_application`\n",
    "\"\"\"\n",
    "previous_application = client.query(query).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE4iDp6yLEK3"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rifYFJ4YLUWQ"
   },
   "source": [
    "Primary Key Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97,
     "status": "ok",
     "timestamp": 1764068556193,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "RMmisR8JLRXQ",
    "outputId": "a43dd5a9-cc30-4367-a67c-63162cb7b5c0"
   },
   "outputs": [],
   "source": [
    "# SK_ID_PREV is unique?\n",
    "\n",
    "print(\"Unique SK_ID_PREV:\", previous_application['SK_ID_PREV'].nunique())\n",
    "print(\"Total rows:\", previous_application.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOjJcntGMOx3"
   },
   "source": [
    "The SK_ID_PREV column is a perfect primary key at the loan-level. Each row represents a unique previous application, with no duplicates among the 1,670,214 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1764068556523,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "WgCyKwKjLgfF"
   },
   "outputs": [],
   "source": [
    "# How many applicatios per customer\n",
    "prev_counts = previous_application.groupby('SK_ID_CURR').size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1764068556594,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "fuN4NCVKMU3P",
    "outputId": "1b260a8c-815d-4fe5-d406-7a2232d17210"
   },
   "outputs": [],
   "source": [
    "print(prev_counts.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8RPPyxSMyv_"
   },
   "source": [
    "Each customer has made multiple previous applications, with an average of ~5 per customer. While most customers have between 2 and 7 applications, some outliers exist with up to 77 applications. This confirms the need for customer-level aggregation to create meaningful features for credit risk modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1764068556818,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "GJgP-EMFNNhP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Nh9KAd-NOK2"
   },
   "source": [
    "# Data Types & Null Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1764068558716,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "tSpLo_2TK7DE",
    "outputId": "c174e633-c3f0-4940-fef6-31233d6ffef1"
   },
   "outputs": [],
   "source": [
    "# Data types\n",
    "print(previous_application.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1764068562273,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "CW5rEXF1LKoh",
    "outputId": "8d8f6443-d8d9-471b-9d6f-54eacac2e860"
   },
   "outputs": [],
   "source": [
    "# Null ratios\n",
    "null_ratio = previous_application.isnull().mean().sort_values(ascending=False)\n",
    "null_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0k9rgUgNtrK"
   },
   "source": [
    "Some columns have extremely high missing rates (over 99%), like RATE_INTEREST_PRIVILEGED and RATE_INTEREST_PRIMARY, which are almost entirely empty and not useful for modeling.\n",
    "\n",
    "Columns like AMT_DOWN_PAYMENT, RATE_DOWN_PAYMENT, NAME_TYPE_SUITE, and DAYS_TERMINATION have 22–53% missing values. These may require imputation or careful handling during feature engineering.\n",
    "\n",
    "Core columns like SK_ID_PREV, SK_ID_CURR, AMT_CREDIT, AMT_APPLICATION, NAME_CONTRACT_TYPE, and DAYS_DECISION are fully populated and suitable for aggregation into customer-level features.\n",
    "\n",
    "This missing value analysis guides which features to aggregate, impute, or exclude, ensuring robust credit risk features at the customer level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KCuPuZFPxDg"
   },
   "source": [
    "Before imputing missing values, we analyze their distribution and meaning. Some missing values represent actual events (like not having a down payment or not terminating a credit), while others are random. This informs how we impute or flag missing data for feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keO2kRXthhrI"
   },
   "source": [
    "# Missing Value & Outlier Analysis & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 116507,
     "status": "ok",
     "timestamp": 1764068678812,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "0NgOtr37h4BZ",
    "outputId": "0d1b8aa6-7519-4089-af1e-68a88fd2e4b3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use the correct dataset name\n",
    "df = previous_application\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Missing value analysis\n",
    "# -----------------------------\n",
    "missing_percent = df.isnull().mean() * 100\n",
    "\n",
    "# Columns with >70% missing → Drop\n",
    "high_missing_cols = missing_percent[missing_percent > 70].index.tolist()\n",
    "print(\"Dropping columns with >70% missing:\")\n",
    "print(high_missing_cols)\n",
    "\n",
    "df_clean = df.drop(columns=high_missing_cols)\n",
    "\n",
    "# Mid missing (20–70%)\n",
    "mid_missing_cols = missing_percent[(missing_percent > 20) & (missing_percent <= 70)].index.tolist()\n",
    "print(\"\\nColumns with 20–70% missing:\")\n",
    "print(mid_missing_cols)\n",
    "\n",
    "# Low missing (0–20%)\n",
    "low_missing_cols = missing_percent[(missing_percent > 0) & (missing_percent <= 20)].index.tolist()\n",
    "print(\"\\nColumns with 0–20% missing:\")\n",
    "print(low_missing_cols)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Function: histogram + boxplot + outlier report for each column\n",
    "# -------------------------------------------------------------\n",
    "def plot_dist_and_outliers(df, cols):\n",
    "    for col in cols:\n",
    "        # Only analyze numeric columns\n",
    "        if df[col].dtype not in ['float64', 'int64']:\n",
    "            print(f\"\\nSkipping non-numeric column: {col}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n==== {col} ====\")\n",
    "\n",
    "        # Plot histogram & boxplot\n",
    "        plt.figure(figsize=(14,5))\n",
    "\n",
    "        # Histogram\n",
    "        plt.subplot(1,2,1)\n",
    "        sns.histplot(df[col], kde=True, bins=50)\n",
    "        plt.title(f\"{col} - Histogram\")\n",
    "\n",
    "        # Boxplot\n",
    "        plt.subplot(1,2,2)\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f\"{col} - Boxplot\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Outlier detection (IQR rule)\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "\n",
    "        outlier_count = df[(df[col] < lower) | (df[col] > upper)].shape[0]\n",
    "        print(f\"Outlier count: {outlier_count}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2. Run EDA for mid-missing numeric columns\n",
    "# ------------------------------------------\n",
    "plot_dist_and_outliers(df_clean, mid_missing_cols)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3. Run EDA for low-missing numeric columns\n",
    "# ------------------------------------------\n",
    "plot_dist_and_outliers(df_clean, low_missing_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52816,
     "status": "ok",
     "timestamp": 1764068731647,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "fH0yRlBYcQsS",
    "outputId": "c30b339d-0a9f-4149-8cc4-0634e20cc097"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# previous_application: Data Cleaning + Feature Engineering Pipeline\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Load dataset: use in-memory DataFrame if exists, else read\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    df = previous_application.copy()\n",
    "    print(\"Using in-memory DataFrame: previous_application\")\n",
    "except NameError:\n",
    "    csv_path = \"/mnt/data/previous_application.csv\"   # <-- this is your uploaded local file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded dataset from {csv_path}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Identify numeric & categorical columns\n",
    "# ------------------------------------------------------------\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Drop columns with >70% missing\n",
    "# ------------------------------------------------------------\n",
    "missing_ratio = df.isna().mean()\n",
    "cols_to_drop = missing_ratio[missing_ratio > 0.70].index.tolist()\n",
    "\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "print(\"Dropped columns (>70% missing):\", cols_to_drop)\n",
    "\n",
    "# refresh lists\n",
    "num_cols = [c for c in num_cols if c not in cols_to_drop]\n",
    "cat_cols = [c for c in cat_cols if c not in cols_to_drop]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Medium-missing columns (20–70%)\n",
    "#    Numeric  → sentinel -999\n",
    "#    Categorical → \"Unknown\"\n",
    "# ------------------------------------------------------------\n",
    "medium_missing = missing_ratio[(missing_ratio > 0.20) & (missing_ratio <= 0.70)].index.tolist()\n",
    "print(\"Medium-missing columns (20–70%):\", medium_missing)\n",
    "\n",
    "for col in medium_missing:\n",
    "    if col in num_cols:\n",
    "        df[col] = df[col].fillna(-999)  # domain-aware sentinel\n",
    "    elif col in cat_cols:\n",
    "        df[col] = df[col].fillna(\"Unknown\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Low-missing columns (<20%)\n",
    "#    Numeric → median\n",
    "#    Categorical → \"Unknown\"\n",
    "# ------------------------------------------------------------\n",
    "low_missing = missing_ratio[(missing_ratio > 0) & (missing_ratio <= 0.20)].index.tolist()\n",
    "print(\"Low-missing columns (0–20%):\", low_missing)\n",
    "\n",
    "for col in low_missing:\n",
    "    if col in num_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    elif col in cat_cols:\n",
    "        df[col] = df[col].fillna(\"Unknown\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Safety pass (ensure no NA left)\n",
    "# ------------------------------------------------------------\n",
    "for col in df.columns:\n",
    "    if df[col].isna().any():\n",
    "        if col in num_cols:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        else:\n",
    "            df[col] = df[col].fillna(\"Unknown\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Outliers\n",
    "#    DO NOT remove outliers — credit data extremes contain real signal.\n",
    "# ------------------------------------------------------------\n",
    "print(\"Outliers preserved (no removal).\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Basic Feature Engineering for previous_application\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Flag: was the application approved?\n",
    "if \"NAME_CONTRACT_STATUS\" in df.columns:\n",
    "    df[\"APPROVED_FLAG\"] = (df[\"NAME_CONTRACT_STATUS\"] == \"Approved\").astype(int) # A loan application being approved or not is one of the strongest behavioral signals for credit scoring/1 if application approved, 0 otherwise\n",
    "\n",
    "# Credit to goods price ratio\n",
    "if \"AMT_CREDIT\" in df.columns and \"AMT_GOODS_PRICE\" in df.columns:\n",
    "    df[\"CREDIT_GOODS_RATIO\"] = df[\"AMT_CREDIT\"] / (df[\"AMT_GOODS_PRICE\"] + 1) # This ratio measures what fraction of the goods price is being financed through credit/A higher ratio often indicates higher financial dependency and therefore higher risk.\n",
    "\n",
    "# Application processing duration example\n",
    "if \"DAYS_DECISION\" in df.columns:\n",
    "    df[\"FAST_DECISION_FLAG\"] = (df[\"DAYS_DECISION\"] > -5).astype(int) # Captures how quickly the lender made a decision.Explanation:DAYS_DECISION close to zero means the institution approved/rejected\n",
    "                                                                        #the application very quickly, which may indicate: pre-approved loans, automated risk flags,high-risk or low-risk customers depending\n",
    "                                                                        # on lending policy.This becomes a useful proxy for lender confidence.\n",
    "\n",
    "# Annuity ratio if exists\n",
    "if \"AMT_ANNUITY\" in df.columns and \"AMT_CREDIT\" in df.columns:\n",
    "    df[\"ANNUITY_CREDIT_RATIO\"] = df[\"AMT_ANNUITY\"] / (df[\"AMT_CREDIT\"] + 1) # Explanation:Represents the monthly burden relative to total loan amount. Higher ratios imply higher payment pressure,\n",
    "                                                                            #which is a strong indicator for repayment capacity and default risk.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8) Convert categorical columns to category for modelling\n",
    "# ------------------------------------------------------------\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9) Output\n",
    "# ------------------------------------------------------------\n",
    "print(\"Final shape:\", df.shape)\n",
    "print(\"Preview columns:\", df.columns[:30].tolist())\n",
    "\n",
    "# assign back to original variable\n",
    "previous_application = df\n",
    "\n",
    "# optional save\n",
    "df.to_csv(\"previous_application_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"Cleaning + feature engineering for previous_application completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbERRwYHjLt3"
   },
   "source": [
    "# Feature Engineering + Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 30559,
     "status": "ok",
     "timestamp": 1764068762153,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "-leLepm-fLgV",
    "outputId": "7cff06c7-5a2f-4c0d-b245-3fabdaa8081c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# 1) Identify variable types\n",
    "# -------------------------\n",
    "\n",
    "numeric_cols = previous_application.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = previous_application.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove customer ID from FE lists\n",
    "numeric_cols = [col for col in numeric_cols if col not in [\"SK_ID_CURR\", \"SK_ID_PREV\"]]\n",
    "categorical_cols = [col for col in categorical_cols if col not in [\"SK_ID_CURR\", \"SK_ID_PREV\"]]\n",
    "\n",
    "# --------------------------------------\n",
    "# 2) Numeric feature aggregation rules\n",
    "# --------------------------------------\n",
    "\n",
    "num_aggs = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    num_aggs[col] = [\"mean\", \"std\", \"min\", \"max\", \"median\"]\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3) Categorical variable encoding (One-Hot + agg)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Convert to string to avoid errors\n",
    "previous_application[categorical_cols] = previous_application[categorical_cols].astype(str)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "previous_ohe = pd.get_dummies(previous_application, columns=categorical_cols, dummy_na=False)\n",
    "\n",
    "# Keep only OHE columns (exclude numeric)\n",
    "ohe_cols = [col for col in previous_ohe.columns if any(cat in col for cat in categorical_cols)]\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4) Aggregate numeric features per client\n",
    "# ---------------------------------------\n",
    "\n",
    "numeric_agg_df = previous_application.groupby(\"SK_ID_CURR\").agg(num_aggs)\n",
    "numeric_agg_df.columns = [\"PREV_\" + \"_\".join(col).upper() for col in numeric_agg_df.columns]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) Aggregate one-hot encoded (categorical) features\n",
    "# -------------------------------------------------\n",
    "\n",
    "categorical_agg_df = previous_ohe.groupby(\"SK_ID_CURR\")[ohe_cols].mean()\n",
    "categorical_agg_df.columns = [\"PREV_\" + col.upper() + \"_MEAN\" for col in categorical_agg_df.columns]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) Combine numeric + categorical customer features\n",
    "# -------------------------------------------------\n",
    "\n",
    "previous_agg = pd.concat([numeric_agg_df, categorical_agg_df], axis=1)\n",
    "\n",
    "print(previous_agg.shape)\n",
    "previous_agg.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bP25-mh0hw0q"
   },
   "source": [
    "The previous_application table consists of loan-level historical credit applications, where each customer may have multiple past requests. To make this information usable for a credit risk model, I transformed the raw loan records into customer-level behavioral features.\n",
    "\n",
    "• I applied domain-aware missing value strategies, removed columns with extremely high missing rates, preserved natural credit outliers, and created engineered features such as approval flags, credit-to-goods ratios, and annuity-to-credit ratios.\n",
    "\n",
    "• Using one-hot encoded category distributions and numeric aggregations (mean, max, std, median), I generated customer-level summary metrics such as approval rates, average past credit amounts, decision speed patterns, and loan purpose usage profiles — all of which are powerful predictors of repayment behavior.\n",
    "\n",
    "• The result is a clean, enriched, customer-level feature set (previous_agg) ready to be merged into the main application dataset, significantly enhancing the overall predictive power of the credit scoring model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 85257,
     "status": "ok",
     "timestamp": 1764068847422,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "AFFd-nRpqMOp"
   },
   "outputs": [],
   "source": [
    "previous_agg.to_csv(\"previous_agg.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1764068847451,
     "user": {
      "displayName": "Halim Can ALBAY",
      "userId": "05890985450551477746"
     },
     "user_tz": -180
    },
    "id": "FwiNgLZmhBb7"
   },
   "outputs": [],
   "source": [
    "# to merge\n",
    "\n",
    "# application_train = application_train.merge(previous_agg, on=\"SK_ID_CURR\", how=\"left\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPO7UgeF3RzIKFhetHHHnf+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
